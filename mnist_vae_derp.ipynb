{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: TkAgg\n"
     ]
    }
   ],
   "source": [
    "%matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from plotting import plotstuff\n",
    "from imp import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "'''This script demonstrates how to build a variational autoencoder with Keras.\n",
    "\n",
    "Reference: \"Auto-Encoding Variational Bayes\" https://arxiv.org/abs/1312.6114\n",
    "'''\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.layers import Input, Dense, Lambda, Flatten, Reshape, BatchNormalization, Dropout, GaussianNoise, GaussianDropout, Activation\n",
    "from keras.layers import Convolution2D, Deconvolution2D, MaxPooling2D\n",
    "from keras.models import Model\n",
    "from keras import backend as K_backend\n",
    "from keras import objectives\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# global\n",
    "nb_epoch = 50\n",
    "\n",
    "class VariationalAutoencoder(object):\n",
    "    def __init__(self, original_dim=784, latent_dim=2, intermediate_dim=256, batch_size=100, epsilon_std=1.0):\n",
    "        #vae params\n",
    "        self.batch_size = batch_size\n",
    "        self.original_dim = original_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.intermediate_dim = intermediate_dim\n",
    "        self.epsilon_std = epsilon_std\n",
    "\n",
    "        x = Input(batch_shape=(batch_size, original_dim), name='x_input')\n",
    "        h = Dense(intermediate_dim, activation='relu', name='h_hidden_relu_1')(x)\n",
    "        self.z_mean = Dense(latent_dim, name='Z_Mean')(h)\n",
    "        self.z_log_var = Dense(latent_dim, name='Z_Log_Var')(h)\n",
    "\n",
    "        # note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "        z = Lambda(self.sampling, output_shape=(latent_dim,))([self.z_mean, self.z_log_var])\n",
    "\n",
    "        # we instantiate these layers separately so as to reuse them later\n",
    "        decoder_h = Dense(intermediate_dim, activation='relu', name='Decoder_H_Relu')\n",
    "        decoder_mean = Dense(original_dim, activation='sigmoid', name='Decoder_Mean_sig')\n",
    "        h_decoded = decoder_h(z)\n",
    "        x_decoded_mean = decoder_mean(h_decoded)\n",
    "\n",
    "\n",
    "        self.model = Model(x, x_decoded_mean)\n",
    "        self.model.compile(optimizer='rmsprop', loss=self.vae_loss)\n",
    "\n",
    "        # build a model to project inputs on the latent space\n",
    "        self.encoder = Model(x, self.z_mean)\n",
    "\n",
    "        # build a digit generator that can sample from the learned distribution\n",
    "        decoder_input = Input(shape=(latent_dim,))\n",
    "        _h_decoded = decoder_h(decoder_input)\n",
    "        _x_decoded_mean = decoder_mean(_h_decoded)\n",
    "        self.generator = Model(decoder_input, _x_decoded_mean)\n",
    "\n",
    "\n",
    "    def vae_loss(self, x, x_decoded_mean):\n",
    "        xent_loss = self.original_dim * objectives.binary_crossentropy(x, x_decoded_mean)\n",
    "        kl_loss = - 0.5 * K_backend.sum(1 + self.z_log_var - K_backend.square(self.z_mean) - K_backend.exp(self.z_log_var), axis=-1)\n",
    "        return xent_loss + kl_loss\n",
    "\n",
    "\n",
    "    def sampling(self, z_args):\n",
    "        \"Unpacks the tuple input and conducts probabilistic sampling\"\n",
    "        z_mean, z_log_var = z_args\n",
    "        epsilon = K_backend.random_normal(shape=(self.batch_size, self.latent_dim), mean=0.,\n",
    "                                  std=self.epsilon_std)\n",
    "        return z_mean + K_backend.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "    def fit(self, x, y, batch_size=None, nb_epoch=10, verbose=1, callbacks=[], validation_split=0.,\n",
    "            validation_data=None, shuffle=True, class_weight=None, sample_weight=None):\n",
    "        callbacks_history = self.model.fit(x, y, batch_size, nb_epoch, verbose, callbacks, validation_split,\n",
    "                                           validation_data, shuffle, class_weight, sample_weight)\n",
    "        return callbacks_history\n",
    "\n",
    "\n",
    "vaeclass = VariationalAutoencoder(latent_dim=2)\n",
    "vae = vaeclass.model\n",
    "encoder = vaeclass.encoder\n",
    "generator = vaeclass.generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ==== dataset handling - train the VAE on MNIST digits ===\n",
    "batch_size = 100\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import keras.utils.visualize_util\n",
    "keras.utils.visualize_util.plot(vae, 'vae.png', show_shapes=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 5s - loss: 191.2059 - val_loss: 173.8629\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f68003a7b90>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assert 0, 'pause'\n",
    "vae.fit(x_train, x_train,\n",
    "        shuffle=True,\n",
    "        nb_epoch=1,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 2)\n"
     ]
    }
   ],
   "source": [
    "# ===== plotting encoder output\n",
    "# display a 2D plot of the digit classes in the latent space\n",
    "reload(plotstuff)\n",
    "x_test_encoded = encoder.predict(x_test, batch_size=batch_size)\n",
    "print(x_test_encoded.shape)\n",
    "# plotstuff.Easy3dScatter(plt, x_test_encoded, '', s=10, c=y_test )\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], c=y_test)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ====== plotting decoder from latent space =======\n",
    "# display a 2D manifold of the digits\n",
    "n = 15  # figure with 15x15 digits\n",
    "ss=4\n",
    "digit_size = 28\n",
    "figure = np.zeros((digit_size * n, digit_size * n))\n",
    "# we will sample n points within [-15, 15] standard deviations\n",
    "grid_x = np.linspace(-ss, ss, n)\n",
    "grid_y = np.linspace(-ss, ss, n)\n",
    "\n",
    "for i, yi in enumerate(grid_x):\n",
    "    for j, xi in enumerate(grid_y):\n",
    "        z_sample = np.array([[xi, yi]])\n",
    "        x_decoded = generator.predict(z_sample)\n",
    "        digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "        figure[i * digit_size: (i + 1) * digit_size,\n",
    "               j * digit_size: (j + 1) * digit_size] = digit\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(figure)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test some digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def draw_digit(z_sample, digit_size = 28, n=1):\n",
    "    z_sample = np.array(z_sample).reshape(1,2)\n",
    "    figure = np.zeros((digit_size * n, digit_size * n))\n",
    "    x_decoded = generator.predict(z_sample)\n",
    "    digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "    i, j = 0,0\n",
    "    figure[i * digit_size: (i + 1) * digit_size,\n",
    "           j * digit_size: (j + 1) * digit_size] = digit\n",
    "    plt.figure(figsize=(2, 2))\n",
    "    plt.imshow(figure)\n",
    "    plt.show()\n",
    "    \n",
    "draw_digit((4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert 0, 'halt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build a digit generator that can sample from the learned distribution\n",
    "decoder_input = Input(shape=(latent_dim,))\n",
    "_h_decoded = decoder_h(decoder_input)\n",
    "_x_decoded_mean = decoder_mean(_h_decoded)\n",
    "generator = Model(decoder_input, _x_decoded_mean)\n",
    "\n",
    "# display a 2D manifold of the digits\n",
    "n = 15  # figure with 15x15 digits\n",
    "digit_size = 28\n",
    "figure = np.zeros((digit_size * n, digit_size * n))\n",
    "# we will sample n points within [-15, 15] standard deviations\n",
    "ss = 3\n",
    "grid_x = np.linspace(-ss, ss, n)\n",
    "grid_y = np.linspace(-ss, ss, n)\n",
    "\n",
    "for i, yi in enumerate(grid_x):\n",
    "    for j, xi in enumerate(grid_y):\n",
    "        z_sample = np.array([[xi, yi]])\n",
    "        x_decoded = generator.predict(z_sample)\n",
    "        digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "        figure[i * digit_size: (i + 1) * digit_size,\n",
    "               j * digit_size: (j + 1) * digit_size] = digit\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(figure)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convo VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class ConvoVAE(object):\n",
    "    '''This script demonstrates how to build a variational autoencoder\n",
    "    with Keras and deconvolution layers.\n",
    "\n",
    "    Reference: \"Auto-Encoding Variational Bayes\" https://arxiv.org/abs/1312.6114\n",
    "    '''\n",
    "    def __init__(self, input_shape=(28,28,1), latent_dim=2, intermediate_dim=256, batch_size=100, epsilon_std=1.0, dropout_p=0.1,\n",
    "                sigma=0.2):\n",
    "        # input image dimensions\n",
    "        self.input_shape = input_shape\n",
    "        if len(input_shape) == 3:\n",
    "            self.img_rows, self.img_cols, self.img_chns = input_shape\n",
    "        elif len(input_shape) == 2:\n",
    "            self.img_rows, self.img_cols = input_shape\n",
    "            self.img_chns = 1\n",
    "        else:\n",
    "            raise IndexError(\"Invalid shape: {}\".format(input_shape))\n",
    "        self.batch_size = batch_size\n",
    "        self.original_dim = np.prod(input_shape)\n",
    "        self.latent_dim = latent_dim\n",
    "        self.intermediate_dim = intermediate_dim\n",
    "        self.epsilon_std = epsilon_std\n",
    "        n_pool = 2\n",
    "\n",
    "        # number of convolutional filters to use\n",
    "        nb_filters = 64\n",
    "        # convolution kernel size\n",
    "        nb_conv = 3\n",
    "\n",
    "        batch_size = 100\n",
    "        if K_backend.image_dim_ordering() == 'th':\n",
    "            self.original_img_size = (self.img_chns, self.img_rows, self.img_cols)\n",
    "        else:\n",
    "            self.original_img_size = (self.img_rows, self.img_cols, self.img_chns)\n",
    "\n",
    "        x = Input(batch_shape=(batch_size,) + self.original_img_size)\n",
    "        x_a = GaussianNoise(sigma)(x)\n",
    "        x_a = GaussianDropout(dropout_p)(x_a)\n",
    "\n",
    "#         x_a = Activation('linear')(x)\n",
    "        conv_1 = Convolution2D(self.img_chns, 2, 2, border_mode='same', activation='relu')(x_a)\n",
    "        conv_2 = Convolution2D(nb_filters,    2, 2, border_mode='same', activation='relu', subsample=(2, 2))(conv_1)\n",
    "        conv_3 = Convolution2D(nb_filters, nb_conv, nb_conv, border_mode='same', activation='relu', subsample=(1, 1))(conv_2)\n",
    "        conv_3 = MaxPooling2D((n_pool, n_pool))(conv_3)\n",
    "        for i in range(5):\n",
    "            conv_3 = BatchNormalization()(conv_3)\n",
    "            conv_3 = Dropout(dropout_p)(conv_3)\n",
    "            conv_3 = Convolution2D(nb_filters, nb_conv, nb_conv, border_mode='same', activation='relu', subsample=(1, 1))(conv_3)\n",
    "        \n",
    "      \n",
    "        conv_3 = BatchNormalization()(conv_3)\n",
    "        conv_4 = Convolution2D(nb_filters, nb_conv, nb_conv, border_mode='same', activation='relu', subsample=(1, 1))(conv_3)\n",
    "        flat = Flatten()(conv_4)\n",
    "        hidden = Dense(intermediate_dim, activation='relu')(flat)\n",
    "\n",
    "        self.z_mean = Dense(latent_dim)(hidden)\n",
    "        self.z_log_var = Dense(latent_dim)(hidden)\n",
    "\n",
    "        # note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "        # so you could write `Lambda(sampling)([z_mean, z_log_var])`\n",
    "        z = Lambda(self.sampling, output_shape=(latent_dim,))([self.z_mean, self.z_log_var])\n",
    "\n",
    "        # we instantiate these layers separately so as to reuse them later\n",
    "        decoder_hid = Dense(intermediate_dim, activation='relu')\n",
    "        decoder_upsample = Dense(nb_filters * 14 * 14, activation='relu')\n",
    "\n",
    "        if K_backend.image_dim_ordering() == 'th':\n",
    "            output_shape = (batch_size, nb_filters, 14, 14)\n",
    "        else:\n",
    "            output_shape = (batch_size, 14, 14, nb_filters)\n",
    "\n",
    "        decoder_reshape = Reshape(output_shape[1:])\n",
    "        decoder_deconv_1 = Deconvolution2D(nb_filters, nb_conv, nb_conv, output_shape,\n",
    "                                           border_mode='same', subsample=(1, 1), activation='relu')\n",
    "        decoder_deconv_2 = Deconvolution2D(nb_filters, nb_conv, nb_conv, output_shape,\n",
    "                                           border_mode='same', subsample=(1, 1),activation='relu')\n",
    "        if K_backend.image_dim_ordering() == 'th':\n",
    "            output_shape = (batch_size, nb_filters, 29, 29)\n",
    "        else:\n",
    "            output_shape = (batch_size, 29, 29, nb_filters)\n",
    "        decoder_deconv_3_upsamp = Deconvolution2D(nb_filters, 2, 2, output_shape,\n",
    "                                                  border_mode='valid', subsample=(2, 2), activation='relu')\n",
    "        decoder_mean_squash = Convolution2D(self.img_chns, 2, 2, border_mode='valid', activation='sigmoid')\n",
    "\n",
    "        hid_decoded = decoder_hid(z)\n",
    "        up_decoded = decoder_upsample(hid_decoded)\n",
    "        reshape_decoded = decoder_reshape(up_decoded)\n",
    "        deconv_1_decoded = decoder_deconv_1(reshape_decoded)\n",
    "        deconv_2_decoded = decoder_deconv_2(deconv_1_decoded)\n",
    "        x_decoded_relu = decoder_deconv_3_upsamp(deconv_2_decoded)\n",
    "        x_decoded_mean_squash = decoder_mean_squash(x_decoded_relu)\n",
    "\n",
    "        self.model = Model(x, x_decoded_mean_squash)\n",
    "        self.model.compile(optimizer='rmsprop', loss=self.vae_loss)\n",
    "        # self.model.summary()\n",
    "\n",
    "        # build a model to project inputs on the latent space\n",
    "        self.encoder = Model(x, self.z_mean)\n",
    "\n",
    "        # build a digit generator that can sample from the learned distribution\n",
    "        # todo: (un)roll this\n",
    "        decoder_input = Input(shape=(latent_dim,))\n",
    "        _hid_decoded = decoder_hid(decoder_input)\n",
    "        _up_decoded = decoder_upsample(_hid_decoded)\n",
    "        _reshape_decoded = decoder_reshape(_up_decoded)\n",
    "        _deconv_1_decoded = decoder_deconv_1(_reshape_decoded)\n",
    "        _deconv_2_decoded = decoder_deconv_2(_deconv_1_decoded)\n",
    "        _x_decoded_relu = decoder_deconv_3_upsamp(_deconv_2_decoded)\n",
    "        _x_decoded_mean_squash = decoder_mean_squash(_x_decoded_relu)\n",
    "        self.generator = Model(decoder_input, _x_decoded_mean_squash)\n",
    "\n",
    "    def sampling(self, args):\n",
    "        z_mean, z_log_var = args\n",
    "        epsilon = K_backend.random_normal(shape=(self.batch_size, self.latent_dim),\n",
    "                                  mean=0., std=self.epsilon_std)\n",
    "        return z_mean + K_backend.exp(z_log_var) * epsilon\n",
    "\n",
    "    def vae_loss(self, x, x_decoded_mean):\n",
    "        # NOTE: binary_crossentropy expects a batch_size by dim\n",
    "        # for x and x_decoded_mean, so we MUST flatten these!\n",
    "        x = K_backend.flatten(x)\n",
    "        x_decoded_mean = K_backend.flatten(x_decoded_mean)\n",
    "        xent_loss = self.img_rows * self.img_cols * objectives.binary_crossentropy(x, x_decoded_mean)\n",
    "        kl_loss = - 0.5 * K_backend.mean(1 + self.z_log_var - K_backend.square(self.z_mean) - K_backend.exp(self.z_log_var), axis=-1)\n",
    "        return xent_loss + kl_loss\n",
    "\n",
    "    def fit(self, x, y, batch_size=None, nb_epoch=10, verbose=1, callbacks=[], validation_split=0.,\n",
    "            validation_data=None, shuffle=True, class_weight=None, sample_weight=None):\n",
    "        callbacks_history = self.model.fit(x, y, batch_size, nb_epoch, verbose, callbacks, validation_split,\n",
    "                                           validation_data, shuffle, class_weight, sample_weight)\n",
    "        return callbacks_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    \n",
    "vaeclass = ConvoVAE(latent_dim=2)\n",
    "vae = vaeclass.model\n",
    "encoder = vaeclass.encoder\n",
    "generator = vaeclass.generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "x_input (InputLayer)             (100, 784)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "h_hidden_relu_1 (Dense)          (100, 256)            200960      x_input[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "Z_Mean (Dense)                   (100, 2)              514         h_hidden_relu_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "Z_Log_Var (Dense)                (100, 2)              514         h_hidden_relu_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)                (100, 2)              0           Z_Mean[0][0]                     \n",
      "                                                                   Z_Log_Var[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "Decoder_H_Relu (Dense)           multiple              768         lambda_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "Decoder_Mean_sig (Dense)         multiple              201488      Decoder_H_Relu[0][0]             \n",
      "====================================================================================================\n",
      "Total params: 404244\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keras.utils.visualize_util.plot(vae, 'vaec.png', show_shapes=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'VariationalAutoencoder' object has no attribute 'original_img_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-fc863ddc8730>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# train the VAE on MNIST digits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moriginal_img_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvaeclass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moriginal_img_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_img_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'VariationalAutoencoder' object has no attribute 'original_img_size'"
     ]
    }
   ],
   "source": [
    "# train the VAE on MNIST digits\n",
    "original_img_size = vaeclass.original_img_size\n",
    "(x_train, _), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "print(original_img_size)\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_train = x_train.reshape((x_train.shape[0],) + original_img_size)\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_test = x_test.reshape((x_test.shape[0],) + original_img_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 102s - loss: 140.6787 - val_loss: 138.1703\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 102s - loss: 139.4992 - val_loss: 138.3074\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 102s - loss: 138.6479 - val_loss: 136.8518\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 101s - loss: 138.1657 - val_loss: 137.5399\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 101s - loss: 137.6486 - val_loss: 136.7468\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 101s - loss: 137.3305 - val_loss: 136.6298\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 101s - loss: 136.7190 - val_loss: 135.4392\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 101s - loss: 136.3319 - val_loss: 135.6504\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 101s - loss: 136.9648 - val_loss: 136.8793\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 101s - loss: 136.2357 - val_loss: 136.8218\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 101s - loss: 136.1037 - val_loss: 136.9026\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 101s - loss: 137.1677 - val_loss: 133.9673\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 101s - loss: 135.9912 - val_loss: 137.1759\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 101s - loss: 136.3534 - val_loss: 135.1779\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 101s - loss: 136.0372 - val_loss: 133.8939\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 101s - loss: 135.7943 - val_loss: 135.0955\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 101s - loss: 136.1665 - val_loss: 133.8203\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 101s - loss: 136.8744 - val_loss: 135.6907\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 101s - loss: 136.8663 - val_loss: 135.9355\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 101s - loss: 136.8580 - val_loss: 135.4104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbce79f3a10>"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae.fit(x_train, x_train,\n",
    "        shuffle=True,\n",
    "        nb_epoch=1,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 2)\n"
     ]
    }
   ],
   "source": [
    "# display a 2D plot of the digit classes in the latent space\n",
    "x_test_encoded = encoder.predict(x_test, batch_size=batch_size)\n",
    "print(x_test_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], c=y_test)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for axis 1 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-fc9dc5ad7bde>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx_test_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplotstuff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEasy3dScatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test_encoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_test\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/mike/ve/ml/eegkaggle/plotting/plotstuff.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, plotter, data, title, s, figsize, ElevAzi, c)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for axis 1 with size 2"
     ]
    }
   ],
   "source": [
    "x_test_encoded = encoder.predict(x_test, batch_size=batch_size)\n",
    "plotstuff.Easy3dScatter(plt, x_test_encoded, '', s=10, c=y_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# display a 2D manifold of the digits\n",
    "n = 15  # figure with 15x15 digits\n",
    "ss = 4\n",
    "digit_size = 28\n",
    "figure = np.zeros((digit_size * n, digit_size * n))\n",
    "# we will sample n points within [-15, 15] standard deviations\n",
    "grid_x = np.linspace(-ss, ss, n)\n",
    "grid_y = np.linspace(-ss, ss, n)\n",
    "\n",
    "for i, yi in enumerate(grid_x):\n",
    "    for j, xi in enumerate(grid_y):\n",
    "        z_sample = np.array([[xi, yi]])\n",
    "        z_sample = np.tile(z_sample, batch_size).reshape(batch_size, 2)\n",
    "        x_decoded = generator.predict(z_sample, batch_size=batch_size)\n",
    "        digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "        figure[i * digit_size: (i + 1) * digit_size,\n",
    "               j * digit_size: (j + 1) * digit_size] = digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(figure, origin='upper')\n",
    "wid = n*digit_size\n",
    "# plt.scatter(x_test_encoded[:, 0]*wid/4 + wid//2, x_test_encoded[:, 1]*wid/4 + wid//2, c=y_test, alpha=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crossfire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Crossfire(object):\n",
    "    \"\"\"\n",
    "    Covolutional VAE with a \"crossfire\" component - a classifier is bolted onto the end of the encoder and loss function\n",
    "    can be parameterized to function from classifier loss instead of autoencoder loss. Ideally, the model starts in\n",
    "    pure autoencoder mode to learn features, then as loss flattens out the network starts weighing classifier loss\n",
    "    more heavily.\n",
    "    Note to self: things to try:\n",
    "    * Add burst /batch / epoch noise to input\n",
    "    * Modularize out the activation from dropout ordering\"\"\"\n",
    "\n",
    "    def __init__(self, input_shape=(28, 28, 1), latent_dim=2, intermediate_dim=256, batch_size=100, epsilon_std=1.0,\n",
    "                 dropout_p=0.1, n_stax=0, n_classes=10):\n",
    "        # input image dimensions\n",
    "        self.input_shape = input_shape\n",
    "        if len(input_shape) == 3:\n",
    "            self.img_rows, self.img_cols, self.img_chns = input_shape\n",
    "        elif len(input_shape) == 2:\n",
    "            self.img_rows, self.img_cols = input_shape\n",
    "            self.img_chns = 1\n",
    "        else:\n",
    "            raise IndexError(\"Invalid shape: {}\".format(input_shape))\n",
    "        self.batch_size = batch_size\n",
    "        self.original_dim = np.prod(input_shape)\n",
    "        self.latent_dim = latent_dim\n",
    "        self.intermediate_dim = intermediate_dim\n",
    "        self.epsilon_std = epsilon_std\n",
    "        self.epsilon_ce = 1.0e-9\n",
    "\n",
    "        # number of convolutional filters to use\n",
    "        nb_filters = 64\n",
    "        # convolution kernel size\n",
    "        nb_conv = 3\n",
    "\n",
    "        batch_size = 100\n",
    "        if K_backend.image_dim_ordering() == 'th':\n",
    "            self.original_img_size = (self.img_chns, self.img_rows, self.img_cols)\n",
    "        else:\n",
    "            self.original_img_size = (self.img_rows, self.img_cols, self.img_chns)\n",
    "\n",
    "        x = Input(batch_shape=(batch_size,) + self.original_img_size, name='main_input')\n",
    "        conv_1 = Convolution2D(self.img_chns, 2, 2, border_mode='same', activation='relu')(x)\n",
    "        conv_2 = Convolution2D(nb_filters, 2, 2,\n",
    "                               border_mode='same', activation='relu', subsample=(2, 2))(conv_1)\n",
    "        conv_3 = Convolution2D(nb_filters, nb_conv, nb_conv, border_mode='same', activation='relu', subsample=(1, 1))(\n",
    "            conv_2)\n",
    "        for i in range(n_stax):\n",
    "            conv_3 = BatchNormalization()(conv_3)\n",
    "            conv_3 = Dropout(dropout_p)(conv_3)\n",
    "            conv_3 = Convolution2D(nb_filters, nb_conv, nb_conv, border_mode='same', activation='relu',\n",
    "                                   subsample=(1, 1))(conv_3)\n",
    "\n",
    "        conv_3 = BatchNormalization()(conv_3)\n",
    "        conv_4 = Convolution2D(nb_filters, nb_conv, nb_conv, border_mode='same', activation='relu', subsample=(1, 1))(\n",
    "            conv_3)\n",
    "        flat = Flatten()(conv_4)\n",
    "        hidden = Dense(intermediate_dim, activation='relu', name='intermed')(flat)\n",
    "\n",
    "        self.z_mean = Dense(latent_dim, name='z_mean')(hidden)\n",
    "        self.z_log_var = Dense(latent_dim, name='z_log_var')(hidden)\n",
    "\n",
    "\n",
    "        # note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "        # so you could write `Lambda(sampling)([z_mean, z_log_var])`\n",
    "        z = Lambda(self.sampling, output_shape=(latent_dim,))([self.z_mean, self.z_log_var])\n",
    "\n",
    "        ## ==== End of encoding portion ======\n",
    "\n",
    "        ## ==== Crossfire classifier =========\n",
    "#         c = Lambda(self.crosser, output_shape=(latent_dim,))(self.z_mean, self.z_log_var)\n",
    "        classer = Dense(n_classes, init='normal', activation='softmax', name='classer')(self.z_mean)\n",
    "\n",
    "        # Slicer example\n",
    "        # y = Lambda(lambda x: x[:,0,:,:], output_shape=(1,) + input_shape[2:])(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # we instantiate these layers separately so as to reuse them later\n",
    "        decoder_hid = Dense(intermediate_dim, activation='relu')\n",
    "        decoder_upsample = Dense(nb_filters * 14 * 14, activation='relu')\n",
    "\n",
    "        if K_backend.image_dim_ordering() == 'th':\n",
    "            output_shape = (batch_size, nb_filters, 14, 14)\n",
    "        else:\n",
    "            output_shape = (batch_size, 14, 14, nb_filters)\n",
    "\n",
    "        decoder_reshape = Reshape(output_shape[1:])\n",
    "        decoder_deconv_1 = Deconvolution2D(nb_filters, nb_conv, nb_conv, output_shape, border_mode='same', subsample=(1, 1), activation='relu')\n",
    "        decoder_deconv_2 = Deconvolution2D(nb_filters, nb_conv, nb_conv, output_shape, border_mode='same', subsample=(1, 1), activation='relu')\n",
    "        if K_backend.image_dim_ordering() == 'th':\n",
    "            output_shape = (batch_size, nb_filters, 29, 29)\n",
    "        else:\n",
    "            output_shape = (batch_size, 29, 29, nb_filters)\n",
    "        decoder_deconv_3_upsamp = Deconvolution2D(nb_filters, 2, 2, output_shape,\n",
    "                                                  border_mode='valid', subsample=(2, 2), activation='relu')\n",
    "        decoder_mean_squash = Convolution2D(self.img_chns, 2, 2, border_mode='valid', \n",
    "                                            activation='sigmoid', name='main_output')\n",
    "\n",
    "        hid_decoded = decoder_hid(z)\n",
    "        up_decoded = decoder_upsample(hid_decoded)\n",
    "        reshape_decoded = decoder_reshape(up_decoded)\n",
    "        deconv_1_decoded = decoder_deconv_1(reshape_decoded)\n",
    "        deconv_2_decoded = decoder_deconv_2(deconv_1_decoded)\n",
    "        x_decoded_relu = decoder_deconv_3_upsamp(deconv_2_decoded)\n",
    "        x_decoded_mean_squash = decoder_mean_squash(x_decoded_relu)\n",
    "\n",
    "        \n",
    "\n",
    "        # build a digit generator that can sample from the learned distribution\n",
    "        # todo: (un)roll this\n",
    "        decoder_input = Input(shape=(latent_dim,))\n",
    "        _hid_decoded = decoder_hid(decoder_input)\n",
    "        _up_decoded = decoder_upsample(_hid_decoded)\n",
    "        _reshape_decoded = decoder_reshape(_up_decoded)\n",
    "        _deconv_1_decoded = decoder_deconv_1(_reshape_decoded)\n",
    "        _deconv_2_decoded = decoder_deconv_2(_deconv_1_decoded)\n",
    "        _x_decoded_relu = decoder_deconv_3_upsamp(_deconv_2_decoded)\n",
    "        _x_decoded_mean_squash = decoder_mean_squash(_x_decoded_relu)\n",
    "        \n",
    "\n",
    "        \n",
    "        # Generate models\n",
    "        # Primary model - VAE\n",
    "        self.model = Model(x, x_decoded_mean_squash)\n",
    "        # Crossfilre network\n",
    "        self.classifier = Model(x, classer)\n",
    "        # Ok, now comes the tricky part. See these references:\n",
    "        # https://keras.io/getting-started/functional-api-guide/#multi-input-and-multi-output-models\n",
    "        self.crossmodel = Model(input=x, output=[x_decoded_mean_squash, classer])\n",
    "        self.crossmodel.compile(optimizer='rmsprop',\n",
    "                               loss={'main_output': self.vae_loss,\n",
    "                                    'classer': 'categorical_crossentropy'},\n",
    "                               loss_weights={'main_output': 1.0,\n",
    "                                            'classer': 5.0})\n",
    "        \n",
    "        # build a model to project inputs on the latent space\n",
    "        self.encoder = Model(x, self.z_mean)\n",
    "        # reconstruct digits from latent space        \n",
    "        self.generator = Model(decoder_input, _x_decoded_mean_squash)\n",
    "        self.model.compile(optimizer='rmsprop', loss=self.vae_loss)\n",
    "        self.classifier.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#         self.classifier.compile(loss=self.custom_crossent, optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "        \n",
    "    def fit_crossmodel(self, x_dict, y_dict, batch_size=None, nb_epoch=10, verbose=1, callbacks=[], validation_split=0.,\n",
    "            validation_data=None, shuffle=True, class_weight=None, sample_weight=None):\n",
    "        callbacks_history = self.crossmodel.fit(x_dict, y_dict, batch_size, nb_epoch, verbose, callbacks, validation_split,\n",
    "                                           validation_data, shuffle, class_weight, sample_weight)\n",
    "        return callbacks_history\n",
    "\n",
    "    def sampling(self, args):\n",
    "        z_mean, z_log_var = args\n",
    "        epsilon = K_backend.random_normal(shape=(self.batch_size, self.latent_dim),\n",
    "                                          mean=0., std=self.epsilon_std)\n",
    "        return z_mean + K_backend.exp(z_log_var) * epsilon\n",
    "    \n",
    "    def crosser(self, args):\n",
    "        z_mean, z_log_var = args\n",
    "        return z_mean\n",
    "\n",
    "    def vae_loss(self, x, x_decoded_mean):\n",
    "        # NOTE: binary_crossentropy expects a batch_size by dim\n",
    "        # for x and x_decoded_mean, so we MUST flatten these!\n",
    "        x = K_backend.flatten(x)\n",
    "        x_decoded_mean = K_backend.flatten(x_decoded_mean)\n",
    "        xent_loss = self.img_rows * self.img_cols * objectives.binary_crossentropy(x, x_decoded_mean)\n",
    "        kl_loss = - 0.5 * K_backend.mean(1 + self.z_log_var - K_backend.square(self.z_mean) - K_backend.exp(self.z_log_var), axis=-1)\n",
    "        return xent_loss + kl_loss\n",
    "    \n",
    "    def custom_crossent(self, y_true, y_pred):\n",
    "        '''Just another crossentropy'''\n",
    "        \n",
    "#         y_pred = K_backend.clip(y_pred, self.epsilon_ce, 1.0 - self.epsilon_ce)\n",
    "#         y_pred /= K_backend.sum(y_pred, axis=-1, keepdims=True)\n",
    "        cce = K_backend.categorical_crossentropy(y_pred, y_true)\n",
    "        return cce\n",
    "    \n",
    "    \n",
    "\n",
    "    def fit(self, x, y, batch_size=None, nb_epoch=10, verbose=1, callbacks=[], validation_split=0.,\n",
    "            validation_data=None, shuffle=True, class_weight=None, sample_weight=None):\n",
    "        callbacks_history = self.model.fit(x, y, batch_size, nb_epoch, verbose, callbacks, validation_split,\n",
    "                                           validation_data, shuffle, class_weight, sample_weight)\n",
    "        return callbacks_history\n",
    "\n",
    "    def fit_ae(self, x, batch_size=None, nb_epoch=10, verbose=1, callbacks=[], validation_split=0.,\n",
    "            validation_data=None, shuffle=True, class_weight=None, sample_weight=None):\n",
    "        callbacks_history = self.model.fit(x, x, batch_size, nb_epoch, verbose, callbacks, validation_split,\n",
    "                                           validation_data, shuffle, class_weight, sample_weight)\n",
    "        return callbacks_history\n",
    "    \n",
    "    def crossfit(self, x, y, batch_size=None, nb_epoch_ae=5, nb_epoch_cf=10, verbose=1, callbacks=[], validation_split=0.,\n",
    "            validation_data=None, shuffle=True, class_weight=None, sample_weight=None):\n",
    "        valid_ae, valid_cf = validation_data\n",
    "        for i in range(nb_epoch_ae):\n",
    "            callbacks_history = self.model.fit(x, x, batch_size, 1, verbose, callbacks, validation_split,\n",
    "                                           valid_ae, shuffle, class_weight, sample_weight)\n",
    "        for i in range(nb_epoch_cf):\n",
    "            callbacks_history = self.classifier.fit(x, y, batch_size, 1, verbose, callbacks, validation_split,\n",
    "                                           valid_cf, shuffle, class_weight, sample_weight)\n",
    "        return callbacks_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vaeclass = Crossfire(latent_dim=6, n_stax=3)\n",
    "vae = vaeclass.model\n",
    "encoder = vaeclass.encoder\n",
    "generator = vaeclass.generator\n",
    "classifier = vaeclass.classifier\n",
    "crosser = vaeclass.crossmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import keras.utils.visualize_util\n",
    "keras.utils.visualize_util.plot(vae, 'vaex.png', show_shapes=True)\n",
    "keras.utils.visualize_util.plot(crosser, 'vaex_cross.png', show_shapes=True)\n",
    "keras.utils.visualize_util.plot(classifier, 'vaex_cls.png', show_shapes=True)\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# train the VAE on MNIST digits\n",
    "original_img_size = vaeclass.original_img_size\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "batch_size = 100\n",
    "print(original_img_size)\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_train = x_train.reshape((x_train.shape[0],) + original_img_size)\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_test = x_test.reshape((x_test.shape[0],) + original_img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((60000,), (10000, 10))\n",
      "('x_train.shape:', (60000, 28, 28, 1))\n",
      "('x_train.shape:', (60000, 28, 28, 1))\n"
     ]
    }
   ],
   "source": [
    "# one hot encode outputs\n",
    "y_train_oh = np_utils.to_categorical(y_train)\n",
    "y_test_oh = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test_oh.shape[1]\n",
    "print(y_train.shape, y_test_oh.shape)\n",
    "print('x_train.shape:', x_train.shape)\n",
    "print('x_train.shape:', x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important note: it looks like the model should be trained 1 epoch on VAE only, then 5ish on mixed, and then the remainder on classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('shapes: ', (60000, 28, 28, 1), (10000, 28, 28, 1))\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "11800/60000 [====>.........................] - ETA: 99s - loss: 126.7520"
     ]
    }
   ],
   "source": [
    "print('shapes: ', x_train.shape, x_test.shape)\n",
    "vae.fit(x_train, x_train,\n",
    "        shuffle=True,\n",
    "        nb_epoch=1,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 133s - loss: 161.6293 - main_output_loss: 154.9479 - classer_loss: 1.3363 - val_loss: 117.3646 - val_main_output_loss: 114.1953 - val_classer_loss: 0.6339\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 130s - loss: 113.1073 - main_output_loss: 110.6281 - classer_loss: 0.4958 - val_loss: 107.7050 - val_main_output_loss: 105.8552 - val_classer_loss: 0.3700\n",
      "Epoch 3/5\n",
      " 3100/60000 [>.............................] - ETA: 118s - loss: 108.8257 - main_output_loss: 106.9978 - classer_loss: 0.3656"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-132-8ccd20ab8aba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m vaeclass.fit_crossmodel(x_train, {'main_output': x_train, 'classer': y_train_oh}, \n\u001b[1;32m      2\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'main_output'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'classer'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0my_test_oh\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         nb_epoch=5)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-129-9d45cfab3ce3>\u001b[0m in \u001b[0;36mfit_crossmodel\u001b[0;34m(self, x_dict, y_dict, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight)\u001b[0m\n\u001b[1;32m    146\u001b[0m             validation_data=None, shuffle=True, class_weight=None, sample_weight=None):\n\u001b[1;32m    147\u001b[0m         callbacks_history = self.crossmodel.fit(x_dict, y_dict, batch_size, nb_epoch, verbose, callbacks, validation_split,\n\u001b[0;32m--> 148\u001b[0;31m                                            validation_data, shuffle, class_weight, sample_weight)\n\u001b[0m\u001b[1;32m    149\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcallbacks_history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mike/ve/ml/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight)\u001b[0m\n\u001b[1;32m   1122\u001b[0m                               \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m                               callback_metrics=callback_metrics)\n\u001b[0m\u001b[1;32m   1125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mike/ve/ml/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics)\u001b[0m\n\u001b[1;32m    840\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 842\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    843\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mike/ve/ml/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0mupdated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mike/ve/ml/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 717\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    718\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mike/ve/ml/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    913\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 915\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    916\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mike/ve/ml/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 965\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/mike/ve/ml/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    970\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    973\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mike/ve/ml/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    952\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    953\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vaeclass.fit_crossmodel(x_train, {'main_output': x_train, 'classer': y_train_oh}, \n",
    "        shuffle=True, batch_size=100, validation_data=(x_test, {'main_output':x_test, 'classer':y_test_oh}), \n",
    "        nb_epoch=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# vaeclass.crossfit(x_train, y_train_oh, shuffle=True, batch_size=100, validation_data=((x_test, x_test), (x_test, y_test_oh)), \n",
    "#         nb_epoch_ae=1, nb_epoch_cf=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 83s - loss: 0.0682 - acc: 0.9799 - val_loss: 0.0485 - val_acc: 0.9861\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 80s - loss: 0.0304 - acc: 0.9908 - val_loss: 0.0383 - val_acc: 0.9892\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 80s - loss: 0.0216 - acc: 0.9934 - val_loss: 0.0428 - val_acc: 0.9875\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 81s - loss: 0.0137 - acc: 0.9957 - val_loss: 0.0369 - val_acc: 0.9909\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 81s - loss: 0.0143 - acc: 0.9955 - val_loss: 0.0338 - val_acc: 0.9907\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8dfe421510>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(x_train, y_train_oh, shuffle=True,batch_size=100, validation_data=(x_test, y_test_oh), nb_epoch=5,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 6)\n"
     ]
    }
   ],
   "source": [
    "# display a 2D plot of the digit classes in the latent space\n",
    "x_test_encoded = encoder.predict(x_test, batch_size=batch_size)\n",
    "print(x_test_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], c=y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 6)\n"
     ]
    }
   ],
   "source": [
    "x_train_encoded = encoder.predict(x_train, batch_size=batch_size)\n",
    "print(x_train_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(x_train_encoded[:, 0], x_train_encoded[:, 1], c=y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<plotting.plotstuff.Easy3dScatter at 0x7f8e03a99c90>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plotstuff.Easy3dScatter(plt, x_test_encoded[:,:3], '', s=10, c=y_test )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train_pred = classifier.predict(x_train, batch_size=batch_size)\n",
    "x_test_pred = classifier.predict(x_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8dfd846b90>]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(np.std(x_test_pred, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8dfd8bf0d0>]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(x_train_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ambig = np.sum(np.greater(x_train_pred, 0.5) == y_train_oh, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.9926833333333338"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(ambig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 4s     \n"
     ]
    }
   ],
   "source": [
    "score = classifier.evaluate(x_test, y_test_oh, batch_size=batch_size )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.064500669146581813, 0.9902000081539154]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 2901,  4201,  5124,  6428,  8849,  8853,  9687, 15324, 21355,\n",
       "        23104, 23956, 25546, 29311, 34771, 37648, 38605, 39031, 39305,\n",
       "        47759, 48991, 52294, 54756, 56372, 56686, 56950, 59312, 59433]),)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ambig = np.where(np.std(x_train_pred, axis=1) < 0.18)\n",
    "ambig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "[   0.  267.  169.  554.    0.    0.    0.    6.    3.    0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f8dfc886250>"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 8853\n",
    "print(y_train[n], np.argmax(x_train_pred[n]))\n",
    "print(np.round(x_train_pred[n]*1000))\n",
    "plt.imshow(x_train[n].reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
