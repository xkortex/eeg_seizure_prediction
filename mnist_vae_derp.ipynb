{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fee4647fdd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from plotting import plotstuff\n",
    "from imp import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "'''This script demonstrates how to build a variational autoencoder with Keras.\n",
    "\n",
    "Reference: \"Auto-Encoding Variational Bayes\" https://arxiv.org/abs/1312.6114\n",
    "'''\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.layers import Input, Dense, Lambda, Flatten, Reshape, BatchNormalization, Dropout, GaussianNoise, GaussianDropout, Activation\n",
    "from keras.layers import Convolution2D, Deconvolution2D, MaxPooling2D\n",
    "from keras.models import Model\n",
    "from keras import backend as K_backend\n",
    "from keras import objectives\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras_tqdm import TQDMCallback, TQDMNotebookCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# global\n",
    "nb_epoch = 50\n",
    "\n",
    "class VariationalAutoencoder(object):\n",
    "    def __init__(self, original_dim=784, latent_dim=2, intermediate_dim=256, batch_size=100, epsilon_std=1.0):\n",
    "        #vae params\n",
    "        self.batch_size = batch_size\n",
    "        self.original_dim = original_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.intermediate_dim = intermediate_dim\n",
    "        self.epsilon_std = epsilon_std\n",
    "\n",
    "        x = Input(batch_shape=(batch_size, original_dim), name='x_input')\n",
    "        h = Dense(intermediate_dim, activation='relu', name='h_hidden_relu_1')(x)\n",
    "        self.z_mean = Dense(latent_dim, name='Z_Mean')(h)\n",
    "        self.z_log_var = Dense(latent_dim, name='Z_Log_Var')(h)\n",
    "\n",
    "        # note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "        z = Lambda(self.sampling, output_shape=(latent_dim,))([self.z_mean, self.z_log_var])\n",
    "\n",
    "        # we instantiate these layers separately so as to reuse them later\n",
    "        decoder_h = Dense(intermediate_dim, activation='relu', name='Decoder_H_Relu')\n",
    "        decoder_mean = Dense(original_dim, activation='sigmoid', name='Decoder_Mean_sig')\n",
    "        h_decoded = decoder_h(z)\n",
    "        x_decoded_mean = decoder_mean(h_decoded)\n",
    "\n",
    "\n",
    "        self.model = Model(x, x_decoded_mean)\n",
    "        self.model.compile(optimizer='rmsprop', loss=self.vae_loss)\n",
    "\n",
    "        # build a model to project inputs on the latent space\n",
    "        self.encoder = Model(x, self.z_mean)\n",
    "\n",
    "        # build a digit generator that can sample from the learned distribution\n",
    "        decoder_input = Input(shape=(latent_dim,))\n",
    "        _h_decoded = decoder_h(decoder_input)\n",
    "        _x_decoded_mean = decoder_mean(_h_decoded)\n",
    "        self.generator = Model(decoder_input, _x_decoded_mean)\n",
    "\n",
    "\n",
    "    def vae_loss(self, x, x_decoded_mean):\n",
    "        xent_loss = self.original_dim * objectives.binary_crossentropy(x, x_decoded_mean)\n",
    "        kl_loss = - 0.5 * K_backend.sum(1 + self.z_log_var - K_backend.square(self.z_mean) - K_backend.exp(self.z_log_var), axis=-1)\n",
    "        return xent_loss + kl_loss\n",
    "\n",
    "\n",
    "    def sampling(self, z_args):\n",
    "        \"Unpacks the tuple input and conducts probabilistic sampling\"\n",
    "        z_mean, z_log_var = z_args\n",
    "        epsilon = K_backend.random_normal(shape=(self.batch_size, self.latent_dim), mean=0.,\n",
    "                                  std=self.epsilon_std)\n",
    "        return z_mean + K_backend.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "    def fit(self, x, y, batch_size=None, nb_epoch=10, verbose=1, callbacks=[], validation_split=0.,\n",
    "            validation_data=None, shuffle=True, class_weight=None, sample_weight=None):\n",
    "        callbacks_history = self.model.fit(x, y, batch_size, nb_epoch, verbose, callbacks, validation_split,\n",
    "                                           validation_data, shuffle, class_weight, sample_weight)\n",
    "        return callbacks_history\n",
    "\n",
    "\n",
    "vaeclass = VariationalAutoencoder(latent_dim=2)\n",
    "vae = vaeclass.model\n",
    "encoder = vaeclass.encoder\n",
    "generator = vaeclass.generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ==== dataset handling - train the VAE on MNIST digits ===\n",
    "batch_size = 100\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import keras.utils.visualize_util\n",
    "keras.utils.visualize_util.plot(vae, 'vae.png', show_shapes=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 3s - loss: 190.6388 - val_loss: 172.5704\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b - ETA: 12s - loss: 271.0466\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b - ETA: 3s - loss: 218.3928\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fee1344eef0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assert 0, 'pause'\n",
    "vae.fit(x_train, x_train,\n",
    "        shuffle=True,\n",
    "        nb_epoch=1,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ===== plotting encoder output\n",
    "# display a 2D plot of the digit classes in the latent space\n",
    "reload(plotstuff)\n",
    "x_test_encoded = encoder.predict(x_test, batch_size=batch_size)\n",
    "print(x_test_encoded.shape)\n",
    "# plotstuff.Easy3dScatter(plt, x_test_encoded, '', s=10, c=y_test )\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], c=y_test)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ====== plotting decoder from latent space =======\n",
    "# display a 2D manifold of the digits\n",
    "n = 15  # figure with 15x15 digits\n",
    "ss=4\n",
    "digit_size = 28\n",
    "figure = np.zeros((digit_size * n, digit_size * n))\n",
    "# we will sample n points within [-15, 15] standard deviations\n",
    "grid_x = np.linspace(-ss, ss, n)\n",
    "grid_y = np.linspace(-ss, ss, n)\n",
    "\n",
    "for i, yi in enumerate(grid_x):\n",
    "    for j, xi in enumerate(grid_y):\n",
    "        z_sample = np.array([[xi, yi]])\n",
    "        x_decoded = generator.predict(z_sample)\n",
    "        digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "        figure[i * digit_size: (i + 1) * digit_size,\n",
    "               j * digit_size: (j + 1) * digit_size] = digit\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(figure)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test some digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def draw_digit(z_sample, digit_size = 28, n=1):\n",
    "    z_sample = np.array(z_sample).reshape(1,2)\n",
    "    figure = np.zeros((digit_size * n, digit_size * n))\n",
    "    x_decoded = generator.predict(z_sample)\n",
    "    digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "    i, j = 0,0\n",
    "    figure[i * digit_size: (i + 1) * digit_size,\n",
    "           j * digit_size: (j + 1) * digit_size] = digit\n",
    "    plt.figure(figsize=(2, 2))\n",
    "    plt.imshow(figure)\n",
    "    plt.show()\n",
    "    \n",
    "draw_digit((4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert 0, 'halt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'latent_dim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-f6b0170adae5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# build a digit generator that can sample from the learned distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdecoder_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0m_h_decoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_h\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0m_x_decoded_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_h_decoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_x_decoded_mean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'latent_dim' is not defined"
     ]
    }
   ],
   "source": [
    "# build a digit generator that can sample from the learned distribution\n",
    "decoder_input = Input(shape=(latent_dim,))\n",
    "_h_decoded = decoder_h(decoder_input)\n",
    "_x_decoded_mean = decoder_mean(_h_decoded)\n",
    "generator = Model(decoder_input, _x_decoded_mean)\n",
    "\n",
    "# display a 2D manifold of the digits\n",
    "n = 15  # figure with 15x15 digits\n",
    "digit_size = 28\n",
    "figure = np.zeros((digit_size * n, digit_size * n))\n",
    "# we will sample n points within [-15, 15] standard deviations\n",
    "ss = 3\n",
    "grid_x = np.linspace(-ss, ss, n)\n",
    "grid_y = np.linspace(-ss, ss, n)\n",
    "\n",
    "for i, yi in enumerate(grid_x):\n",
    "    for j, xi in enumerate(grid_y):\n",
    "        z_sample = np.array([[xi, yi]])\n",
    "        x_decoded = generator.predict(z_sample)\n",
    "        digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "        figure[i * digit_size: (i + 1) * digit_size,\n",
    "               j * digit_size: (j + 1) * digit_size] = digit\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(figure)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convo VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class ConvoVAE(object):\n",
    "    '''This script demonstrates how to build a variational autoencoder\n",
    "    with Keras and deconvolution layers.\n",
    "\n",
    "    Reference: \"Auto-Encoding Variational Bayes\" https://arxiv.org/abs/1312.6114\n",
    "    '''\n",
    "    def __init__(self, input_shape=(28,28,1), latent_dim=2, intermediate_dim=256, batch_size=100, epsilon_std=1.0, dropout_p=0.1,\n",
    "                sigma=0.2):\n",
    "        # input image dimensions\n",
    "        self.input_shape = input_shape\n",
    "        if len(input_shape) == 3:\n",
    "            self.img_rows, self.img_cols, self.img_chns = input_shape\n",
    "        elif len(input_shape) == 2:\n",
    "            self.img_rows, self.img_cols = input_shape\n",
    "            self.img_chns = 1\n",
    "        else:\n",
    "            raise IndexError(\"Invalid shape: {}\".format(input_shape))\n",
    "        self.batch_size = batch_size\n",
    "        self.original_dim = np.prod(input_shape)\n",
    "        self.latent_dim = latent_dim\n",
    "        self.intermediate_dim = intermediate_dim\n",
    "        self.epsilon_std = epsilon_std\n",
    "        n_pool = 2\n",
    "\n",
    "        # number of convolutional filters to use\n",
    "        nb_filters = 64\n",
    "        # convolution kernel size\n",
    "        nb_conv = 3\n",
    "\n",
    "        batch_size = 100\n",
    "        if K_backend.image_dim_ordering() == 'th':\n",
    "            self.original_img_size = (self.img_chns, self.img_rows, self.img_cols)\n",
    "        else:\n",
    "            self.original_img_size = (self.img_rows, self.img_cols, self.img_chns)\n",
    "\n",
    "        x = Input(batch_shape=(batch_size,) + self.original_img_size)\n",
    "        x_a = GaussianNoise(sigma)(x)\n",
    "        x_a = GaussianDropout(dropout_p)(x_a)\n",
    "\n",
    "#         x_a = Activation('linear')(x)\n",
    "        conv_1 = Convolution2D(self.img_chns, 2, 2, border_mode='same', activation='relu')(x_a)\n",
    "        conv_2 = Convolution2D(nb_filters,    2, 2, border_mode='same', activation='relu', subsample=(2, 2))(conv_1)\n",
    "        conv_3 = Convolution2D(nb_filters, nb_conv, nb_conv, border_mode='same', activation='relu', subsample=(1, 1))(conv_2)\n",
    "        conv_3 = MaxPooling2D((n_pool, n_pool))(conv_3)\n",
    "        for i in range(5):\n",
    "            conv_3 = BatchNormalization()(conv_3)\n",
    "            conv_3 = Dropout(dropout_p)(conv_3)\n",
    "            conv_3 = Convolution2D(nb_filters, nb_conv, nb_conv, border_mode='same', activation='relu', subsample=(1, 1))(conv_3)\n",
    "        \n",
    "      \n",
    "        conv_3 = BatchNormalization()(conv_3)\n",
    "        conv_4 = Convolution2D(nb_filters, nb_conv, nb_conv, border_mode='same', activation='relu', subsample=(1, 1))(conv_3)\n",
    "        flat = Flatten()(conv_4)\n",
    "        hidden = Dense(intermediate_dim, activation='relu')(flat)\n",
    "\n",
    "        self.z_mean = Dense(latent_dim)(hidden)\n",
    "        self.z_log_var = Dense(latent_dim)(hidden)\n",
    "\n",
    "        # note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "        # so you could write `Lambda(sampling)([z_mean, z_log_var])`\n",
    "        z = Lambda(self.sampling, output_shape=(latent_dim,))([self.z_mean, self.z_log_var])\n",
    "\n",
    "        # we instantiate these layers separately so as to reuse them later\n",
    "        decoder_hid = Dense(intermediate_dim, activation='relu')\n",
    "        decoder_upsample = Dense(nb_filters * 14 * 14, activation='relu')\n",
    "\n",
    "        if K_backend.image_dim_ordering() == 'th':\n",
    "            output_shape = (batch_size, nb_filters, 14, 14)\n",
    "        else:\n",
    "            output_shape = (batch_size, 14, 14, nb_filters)\n",
    "\n",
    "        decoder_reshape = Reshape(output_shape[1:])\n",
    "        decoder_deconv_1 = Deconvolution2D(nb_filters, nb_conv, nb_conv, output_shape,\n",
    "                                           border_mode='same', subsample=(1, 1), activation='relu')\n",
    "        decoder_deconv_2 = Deconvolution2D(nb_filters, nb_conv, nb_conv, output_shape,\n",
    "                                           border_mode='same', subsample=(1, 1),activation='relu')\n",
    "        if K_backend.image_dim_ordering() == 'th':\n",
    "            output_shape = (batch_size, nb_filters, 29, 29)\n",
    "        else:\n",
    "            output_shape = (batch_size, 29, 29, nb_filters)\n",
    "        decoder_deconv_3_upsamp = Deconvolution2D(nb_filters, 2, 2, output_shape,\n",
    "                                                  border_mode='valid', subsample=(2, 2), activation='relu')\n",
    "        decoder_mean_squash = Convolution2D(self.img_chns, 2, 2, border_mode='valid', activation='sigmoid')\n",
    "\n",
    "        hid_decoded = decoder_hid(z)\n",
    "        up_decoded = decoder_upsample(hid_decoded)\n",
    "        reshape_decoded = decoder_reshape(up_decoded)\n",
    "        deconv_1_decoded = decoder_deconv_1(reshape_decoded)\n",
    "        deconv_2_decoded = decoder_deconv_2(deconv_1_decoded)\n",
    "        x_decoded_relu = decoder_deconv_3_upsamp(deconv_2_decoded)\n",
    "        x_decoded_mean_squash = decoder_mean_squash(x_decoded_relu)\n",
    "\n",
    "        self.model = Model(x, x_decoded_mean_squash)\n",
    "        self.model.compile(optimizer='rmsprop', loss=self.vae_loss)\n",
    "        # self.model.summary()\n",
    "\n",
    "        # build a model to project inputs on the latent space\n",
    "        self.encoder = Model(x, self.z_mean)\n",
    "\n",
    "        # build a digit generator that can sample from the learned distribution\n",
    "        # todo: (un)roll this\n",
    "        decoder_input = Input(shape=(latent_dim,))\n",
    "        _hid_decoded = decoder_hid(decoder_input)\n",
    "        _up_decoded = decoder_upsample(_hid_decoded)\n",
    "        _reshape_decoded = decoder_reshape(_up_decoded)\n",
    "        _deconv_1_decoded = decoder_deconv_1(_reshape_decoded)\n",
    "        _deconv_2_decoded = decoder_deconv_2(_deconv_1_decoded)\n",
    "        _x_decoded_relu = decoder_deconv_3_upsamp(_deconv_2_decoded)\n",
    "        _x_decoded_mean_squash = decoder_mean_squash(_x_decoded_relu)\n",
    "        self.generator = Model(decoder_input, _x_decoded_mean_squash)\n",
    "\n",
    "    def sampling(self, args):\n",
    "        z_mean, z_log_var = args\n",
    "        epsilon = K_backend.random_normal(shape=(self.batch_size, self.latent_dim),\n",
    "                                  mean=0., std=self.epsilon_std)\n",
    "        return z_mean + K_backend.exp(z_log_var) * epsilon\n",
    "\n",
    "    def vae_loss(self, x, x_decoded_mean):\n",
    "        # NOTE: binary_crossentropy expects a batch_size by dim\n",
    "        # for x and x_decoded_mean, so we MUST flatten these!\n",
    "        x = K_backend.flatten(x)\n",
    "        x_decoded_mean = K_backend.flatten(x_decoded_mean)\n",
    "        xent_loss = self.img_rows * self.img_cols * objectives.binary_crossentropy(x, x_decoded_mean)\n",
    "        kl_loss = - 0.5 * K_backend.mean(1 + self.z_log_var - K_backend.square(self.z_mean) - K_backend.exp(self.z_log_var), axis=-1)\n",
    "        return xent_loss + kl_loss\n",
    "\n",
    "    def fit(self, x, y, batch_size=None, nb_epoch=10, verbose=1, callbacks=[], validation_split=0.,\n",
    "            validation_data=None, shuffle=True, class_weight=None, sample_weight=None):\n",
    "        callbacks_history = self.model.fit(x, y, batch_size, nb_epoch, verbose, callbacks, validation_split,\n",
    "                                           validation_data, shuffle, class_weight, sample_weight)\n",
    "        return callbacks_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    \n",
    "vaeclass = ConvoVAE(latent_dim=2)\n",
    "vae = vaeclass.model\n",
    "encoder = vaeclass.encoder\n",
    "generator = vaeclass.generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_4 (InputLayer)             (100, 28, 28, 1)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "gaussiannoise_2 (GaussianNoise)  (100, 28, 28, 1)      0           input_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "gaussiandropout_2 (GaussianDropo (100, 28, 28, 1)      0           gaussiannoise_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_11 (Convolution2D) (100, 28, 28, 1)      5           gaussiandropout_2[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_12 (Convolution2D) (100, 14, 14, 64)     320         convolution2d_11[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_13 (Convolution2D) (100, 14, 14, 64)     36928       convolution2d_12[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_2 (MaxPooling2D)    (100, 7, 7, 64)       0           convolution2d_13[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_7 (BatchNorma (100, 7, 7, 64)       256         maxpooling2d_2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)              (100, 7, 7, 64)       0           batchnormalization_7[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_14 (Convolution2D) (100, 7, 7, 64)       36928       dropout_6[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_8 (BatchNorma (100, 7, 7, 64)       256         convolution2d_14[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)              (100, 7, 7, 64)       0           batchnormalization_8[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_15 (Convolution2D) (100, 7, 7, 64)       36928       dropout_7[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_9 (BatchNorma (100, 7, 7, 64)       256         convolution2d_15[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)              (100, 7, 7, 64)       0           batchnormalization_9[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_16 (Convolution2D) (100, 7, 7, 64)       36928       dropout_8[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_10 (BatchNorm (100, 7, 7, 64)       256         convolution2d_16[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)              (100, 7, 7, 64)       0           batchnormalization_10[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_17 (Convolution2D) (100, 7, 7, 64)       36928       dropout_9[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_11 (BatchNorm (100, 7, 7, 64)       256         convolution2d_17[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)             (100, 7, 7, 64)       0           batchnormalization_11[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_18 (Convolution2D) (100, 7, 7, 64)       36928       dropout_10[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_12 (BatchNorm (100, 7, 7, 64)       256         convolution2d_18[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_19 (Convolution2D) (100, 7, 7, 64)       36928       batchnormalization_12[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (100, 3136)           0           convolution2d_19[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dense_6 (Dense)                  (100, 256)            803072      flatten_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_7 (Dense)                  (100, 2)              514         dense_6[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_8 (Dense)                  (100, 2)              514         dense_6[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)                (100, 2)              0           dense_7[0][0]                    \n",
      "                                                                   dense_8[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_9 (Dense)                  multiple              768         lambda_3[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_10 (Dense)                 multiple              3223808     dense_9[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)              multiple              0           dense_10[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "deconvolution2d_4 (Deconvolution multiple              36928       reshape_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "deconvolution2d_5 (Deconvolution multiple              36928       deconvolution2d_4[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "deconvolution2d_6 (Deconvolution multiple              16448       deconvolution2d_5[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_20 (Convolution2D) multiple              257         deconvolution2d_6[0][0]          \n",
      "====================================================================================================\n",
      "Total params: 4,379,594\n",
      "Trainable params: 4,378,826\n",
      "Non-trainable params: 768\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keras.utils.visualize_util.plot(vae, 'vaec.png', show_shapes=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# train the VAE on MNIST digits\n",
    "original_img_size = vaeclass.original_img_size\n",
    "(x_train, _), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "print(original_img_size)\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_train = x_train.reshape((x_train.shape[0],) + original_img_size)\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_test = x_test.reshape((x_test.shape[0],) + original_img_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vae.fit(x_train, x_train,\n",
    "        shuffle=True,\n",
    "        nb_epoch=1,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(x_test, x_test),\n",
    "        verbose=0,\n",
    "        callbacks=[TQDMNotebookCallback()])\n",
    "time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 2)\n"
     ]
    }
   ],
   "source": [
    "# display a 2D plot of the digit classes in the latent space\n",
    "x_test_encoded = encoder.predict(x_test, batch_size=batch_size)\n",
    "print(x_test_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAg0AAAIFCAYAAABRS875AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nsrM9vhKMAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd1bc1aa978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], c=y_test)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for axis 1 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-fc9dc5ad7bde>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx_test_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplotstuff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEasy3dScatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test_encoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_test\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/mike/py/kaggle/eegkaggle/plotting/plotstuff.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, plotter, data, title, s, figsize, ElevAzi, c)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for axis 1 with size 2"
     ]
    }
   ],
   "source": [
    "x_test_encoded = encoder.predict(x_test, batch_size=batch_size)\n",
    "plotstuff.Easy3dScatter(plt, x_test_encoded, '', s=10, c=y_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# display a 2D manifold of the digits\n",
    "n = 15  # figure with 15x15 digits\n",
    "ss = 4\n",
    "digit_size = 28\n",
    "figure = np.zeros((digit_size * n, digit_size * n))\n",
    "# we will sample n points within [-15, 15] standard deviations\n",
    "grid_x = np.linspace(-ss, ss, n)\n",
    "grid_y = np.linspace(-ss, ss, n)\n",
    "\n",
    "for i, yi in enumerate(grid_x):\n",
    "    for j, xi in enumerate(grid_y):\n",
    "        z_sample = np.array([[xi, yi]])\n",
    "        z_sample = np.tile(z_sample, batch_size).reshape(batch_size, 2)\n",
    "        x_decoded = generator.predict(z_sample, batch_size=batch_size)\n",
    "        digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "        figure[i * digit_size: (i + 1) * digit_size,\n",
    "               j * digit_size: (j + 1) * digit_size] = digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(figure, origin='upper')\n",
    "wid = n*digit_size\n",
    "# plt.scatter(x_test_encoded[:, 0]*wid/4 + wid//2, x_test_encoded[:, 1]*wid/4 + wid//2, c=y_test, alpha=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crossfire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Crossfire(object):\n",
    "    \"\"\"\n",
    "    Covolutional VAE with a \"crossfire\" component - a classifier is bolted onto the end of the encoder and loss function\n",
    "    can be parameterized to function from classifier loss instead of autoencoder loss. Ideally, the model starts in\n",
    "    pure autoencoder mode to learn features, then as loss flattens out the network starts weighing classifier loss\n",
    "    more heavily.\n",
    "    Note to self: things to try:\n",
    "    * Add burst /batch / epoch noise to input\n",
    "    * Modularize out the activation from dropout ordering\"\"\"\n",
    "\n",
    "    def __init__(self, input_shape=(28, 28, 1), latent_dim=2, intermediate_dim=256, batch_size=100, epsilon_std=1.0,\n",
    "                 dropout_p=0.1, n_stax=0, n_classes=10):\n",
    "        # input image dimensions\n",
    "        self.input_shape = input_shape\n",
    "        if len(input_shape) == 3:\n",
    "            self.img_rows, self.img_cols, self.img_chns = input_shape\n",
    "        elif len(input_shape) == 2:\n",
    "            self.img_rows, self.img_cols = input_shape\n",
    "            self.img_chns = 1\n",
    "        else:\n",
    "            raise IndexError(\"Invalid shape: {}\".format(input_shape))\n",
    "        self.batch_size = batch_size\n",
    "        self.original_dim = np.prod(input_shape)\n",
    "        self.latent_dim = latent_dim\n",
    "        self.intermediate_dim = intermediate_dim\n",
    "        self.epsilon_std = epsilon_std\n",
    "        self.epsilon_ce = 1.0e-9\n",
    "\n",
    "        # number of convolutional filters to use\n",
    "        nb_filters = 64\n",
    "        # convolution kernel size\n",
    "        nb_conv = 3\n",
    "\n",
    "        batch_size = 100\n",
    "        if K_backend.image_dim_ordering() == 'th':\n",
    "            self.original_img_size = (self.img_chns, self.img_rows, self.img_cols)\n",
    "        else:\n",
    "            self.original_img_size = (self.img_rows, self.img_cols, self.img_chns)\n",
    "\n",
    "        x = Input(batch_shape=(batch_size,) + self.original_img_size, name='main_input')\n",
    "        conv_1 = Convolution2D(self.img_chns, 2, 2, border_mode='same', activation='relu')(x)\n",
    "        conv_2 = Convolution2D(nb_filters, 2, 2,\n",
    "                               border_mode='same', activation='relu', subsample=(2, 2))(conv_1)\n",
    "        conv_3 = Convolution2D(nb_filters, nb_conv, nb_conv, border_mode='same', activation='relu', subsample=(1, 1))(\n",
    "            conv_2)\n",
    "        for i in range(n_stax):\n",
    "            conv_3 = BatchNormalization()(conv_3)\n",
    "            conv_3 = Dropout(dropout_p)(conv_3)\n",
    "            conv_3 = Convolution2D(nb_filters, nb_conv, nb_conv, border_mode='same', activation='relu',\n",
    "                                   subsample=(1, 1))(conv_3)\n",
    "\n",
    "        conv_3 = BatchNormalization()(conv_3)\n",
    "        conv_4 = Convolution2D(nb_filters, nb_conv, nb_conv, border_mode='same', activation='relu', subsample=(1, 1))(\n",
    "            conv_3)\n",
    "        flat = Flatten()(conv_4)\n",
    "        hidden = Dense(intermediate_dim, activation='relu', name='intermed')(flat)\n",
    "\n",
    "        self.z_mean = Dense(latent_dim, name='z_mean')(hidden)\n",
    "        self.z_log_var = Dense(latent_dim, name='z_log_var')(hidden)\n",
    "\n",
    "\n",
    "        # note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "        # so you could write `Lambda(sampling)([z_mean, z_log_var])`\n",
    "        z = Lambda(self.sampling, output_shape=(latent_dim,))([self.z_mean, self.z_log_var])\n",
    "\n",
    "        ## ==== End of encoding portion ======\n",
    "\n",
    "        ## ==== Crossfire classifier =========\n",
    "#         c = Lambda(self.crosser, output_shape=(latent_dim,))(self.z_mean, self.z_log_var)\n",
    "        classer = Dense(n_classes, init='normal', activation='softmax', name='classer')(self.z_mean)\n",
    "\n",
    "        # Slicer example\n",
    "        # y = Lambda(lambda x: x[:,0,:,:], output_shape=(1,) + input_shape[2:])(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # we instantiate these layers separately so as to reuse them later\n",
    "        decoder_hid = Dense(intermediate_dim, activation='relu')\n",
    "        decoder_upsample = Dense(nb_filters * 14 * 14, activation='relu')\n",
    "\n",
    "        if K_backend.image_dim_ordering() == 'th':\n",
    "            output_shape = (batch_size, nb_filters, 14, 14)\n",
    "        else:\n",
    "            output_shape = (batch_size, 14, 14, nb_filters)\n",
    "\n",
    "        decoder_reshape = Reshape(output_shape[1:])\n",
    "        decoder_deconv_1 = Deconvolution2D(nb_filters, nb_conv, nb_conv, output_shape, border_mode='same', subsample=(1, 1), activation='relu')\n",
    "        decoder_deconv_2 = Deconvolution2D(nb_filters, nb_conv, nb_conv, output_shape, border_mode='same', subsample=(1, 1), activation='relu')\n",
    "        if K_backend.image_dim_ordering() == 'th':\n",
    "            output_shape = (batch_size, nb_filters, 29, 29)\n",
    "        else:\n",
    "            output_shape = (batch_size, 29, 29, nb_filters)\n",
    "        decoder_deconv_3_upsamp = Deconvolution2D(nb_filters, 2, 2, output_shape,\n",
    "                                                  border_mode='valid', subsample=(2, 2), activation='relu')\n",
    "        decoder_mean_squash = Convolution2D(self.img_chns, 2, 2, border_mode='valid', \n",
    "                                            activation='sigmoid', name='main_output')\n",
    "\n",
    "        hid_decoded = decoder_hid(z)\n",
    "        up_decoded = decoder_upsample(hid_decoded)\n",
    "        reshape_decoded = decoder_reshape(up_decoded)\n",
    "        deconv_1_decoded = decoder_deconv_1(reshape_decoded)\n",
    "        deconv_2_decoded = decoder_deconv_2(deconv_1_decoded)\n",
    "        x_decoded_relu = decoder_deconv_3_upsamp(deconv_2_decoded)\n",
    "        x_decoded_mean_squash = decoder_mean_squash(x_decoded_relu)\n",
    "\n",
    "        \n",
    "\n",
    "        # build a digit generator that can sample from the learned distribution\n",
    "        # todo: (un)roll this\n",
    "        decoder_input = Input(shape=(latent_dim,))\n",
    "        _hid_decoded = decoder_hid(decoder_input)\n",
    "        _up_decoded = decoder_upsample(_hid_decoded)\n",
    "        _reshape_decoded = decoder_reshape(_up_decoded)\n",
    "        _deconv_1_decoded = decoder_deconv_1(_reshape_decoded)\n",
    "        _deconv_2_decoded = decoder_deconv_2(_deconv_1_decoded)\n",
    "        _x_decoded_relu = decoder_deconv_3_upsamp(_deconv_2_decoded)\n",
    "        _x_decoded_mean_squash = decoder_mean_squash(_x_decoded_relu)\n",
    "        \n",
    "\n",
    "        \n",
    "        # Generate models\n",
    "        # Primary model - VAE\n",
    "        self.model = Model(x, x_decoded_mean_squash)\n",
    "        # Crossfilre network\n",
    "        self.classifier = Model(x, classer)\n",
    "        # Ok, now comes the tricky part. See these references:\n",
    "        # https://keras.io/getting-started/functional-api-guide/#multi-input-and-multi-output-models\n",
    "        self.crossmodel = Model(input=x, output=[x_decoded_mean_squash, classer])\n",
    "        self.crossmodel.compile(optimizer='rmsprop',\n",
    "                               loss={'main_output': self.vae_loss,\n",
    "                                    'classer': 'categorical_crossentropy'},\n",
    "                               loss_weights={'main_output': 1.0,\n",
    "                                            'classer': 5.0})\n",
    "        \n",
    "        # build a model to project inputs on the latent space\n",
    "        self.encoder = Model(x, self.z_mean)\n",
    "        # reconstruct digits from latent space        \n",
    "        self.generator = Model(decoder_input, _x_decoded_mean_squash)\n",
    "        self.model.compile(optimizer='rmsprop', loss=self.vae_loss)\n",
    "        self.classifier.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#         self.classifier.compile(loss=self.custom_crossent, optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "        \n",
    "    def fit_crossmodel(self, x_dict, y_dict, batch_size=None, nb_epoch=10, verbose=1, callbacks=[], validation_split=0.,\n",
    "            validation_data=None, shuffle=True, class_weight=None, sample_weight=None):\n",
    "        callbacks_history = self.crossmodel.fit(x_dict, y_dict, batch_size, nb_epoch, verbose, callbacks, validation_split,\n",
    "                                           validation_data, shuffle, class_weight, sample_weight)\n",
    "        return callbacks_history\n",
    "\n",
    "    def sampling(self, args):\n",
    "        z_mean, z_log_var = args\n",
    "        epsilon = K_backend.random_normal(shape=(self.batch_size, self.latent_dim),\n",
    "                                          mean=0., std=self.epsilon_std)\n",
    "        return z_mean + K_backend.exp(z_log_var) * epsilon\n",
    "    \n",
    "    def crosser(self, args):\n",
    "        z_mean, z_log_var = args\n",
    "        return z_mean\n",
    "\n",
    "    def vae_loss(self, x, x_decoded_mean):\n",
    "        # NOTE: binary_crossentropy expects a batch_size by dim\n",
    "        # for x and x_decoded_mean, so we MUST flatten these!\n",
    "        x = K_backend.flatten(x)\n",
    "        x_decoded_mean = K_backend.flatten(x_decoded_mean)\n",
    "        xent_loss = self.img_rows * self.img_cols * objectives.binary_crossentropy(x, x_decoded_mean)\n",
    "        kl_loss = - 0.5 * K_backend.mean(1 + self.z_log_var - K_backend.square(self.z_mean) - K_backend.exp(self.z_log_var), axis=-1)\n",
    "        return xent_loss + kl_loss\n",
    "    \n",
    "    def custom_crossent(self, y_true, y_pred):\n",
    "        '''Just another crossentropy'''\n",
    "        \n",
    "#         y_pred = K_backend.clip(y_pred, self.epsilon_ce, 1.0 - self.epsilon_ce)\n",
    "#         y_pred /= K_backend.sum(y_pred, axis=-1, keepdims=True)\n",
    "        cce = K_backend.categorical_crossentropy(y_pred, y_true)\n",
    "        return cce\n",
    "    \n",
    "    \n",
    "\n",
    "    def fit(self, x, y, batch_size=None, nb_epoch=10, verbose=1, callbacks=[], validation_split=0.,\n",
    "            validation_data=None, shuffle=True, class_weight=None, sample_weight=None):\n",
    "        callbacks_history = self.model.fit(x, y, batch_size, nb_epoch, verbose, callbacks, validation_split,\n",
    "                                           validation_data, shuffle, class_weight, sample_weight)\n",
    "        return callbacks_history\n",
    "\n",
    "    def fit_ae(self, x, batch_size=None, nb_epoch=10, verbose=1, callbacks=[], validation_split=0.,\n",
    "            validation_data=None, shuffle=True, class_weight=None, sample_weight=None):\n",
    "        callbacks_history = self.model.fit(x, x, batch_size, nb_epoch, verbose, callbacks, validation_split,\n",
    "                                           validation_data, shuffle, class_weight, sample_weight)\n",
    "        return callbacks_history\n",
    "    \n",
    "    def crossfit(self, x, y, batch_size=None, nb_epoch_ae=5, nb_epoch_cf=10, verbose=1, callbacks=[], validation_split=0.,\n",
    "            validation_data=None, shuffle=True, class_weight=None, sample_weight=None):\n",
    "        valid_ae, valid_cf = validation_data\n",
    "        for i in range(nb_epoch_ae):\n",
    "            callbacks_history = self.model.fit(x, x, batch_size, 1, verbose, callbacks, validation_split,\n",
    "                                           valid_ae, shuffle, class_weight, sample_weight)\n",
    "        for i in range(nb_epoch_cf):\n",
    "            callbacks_history = self.classifier.fit(x, y, batch_size, 1, verbose, callbacks, validation_split,\n",
    "                                           valid_cf, shuffle, class_weight, sample_weight)\n",
    "        return callbacks_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vaeclass = Crossfire(latent_dim=6, n_stax=3)\n",
    "vae = vaeclass.model\n",
    "encoder = vaeclass.encoder\n",
    "generator = vaeclass.generator\n",
    "classifier = vaeclass.classifier\n",
    "crosser = vaeclass.crossmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import keras.utils.visualize_util\n",
    "keras.utils.visualize_util.plot(vae, 'vaex.png', show_shapes=True)\n",
    "keras.utils.visualize_util.plot(crosser, 'vaex_cross.png', show_shapes=True)\n",
    "keras.utils.visualize_util.plot(classifier, 'vaex_cls.png', show_shapes=True)\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train the VAE on MNIST digits\n",
    "original_img_size = vaeclass.original_img_size\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "batch_size = 100\n",
    "print(original_img_size)\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_train = x_train.reshape((x_train.shape[0],) + original_img_size)\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_test = x_test.reshape((x_test.shape[0],) + original_img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# one hot encode outputs\n",
    "y_train_oh = np_utils.to_categorical(y_train)\n",
    "y_test_oh = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test_oh.shape[1]\n",
    "print(y_train.shape, y_test_oh.shape)\n",
    "print('x_train.shape:', x_train.shape)\n",
    "print('x_train.shape:', x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important note: it looks like the model should be trained 1 epoch on VAE only, then 5ish on mixed, and then the remainder on classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('shapes: ', x_train.shape, x_test.shape)\n",
    "vae.fit(x_train, x_train,\n",
    "        shuffle=True,\n",
    "        nb_epoch=1,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(x_test, x_test))\n",
    "time.sleep(0.1) # weird trick to avoid IO crash on progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vaeclass.fit_crossmodel(x_train, {'main_output': x_train, 'classer': y_train_oh}, \n",
    "        shuffle=True, batch_size=100, validation_data=(x_test, {'main_output':x_test, 'classer':y_test_oh}), \n",
    "        nb_epoch=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# vaeclass.crossfit(x_train, y_train_oh, shuffle=True, batch_size=100, validation_data=((x_test, x_test), (x_test, y_test_oh)), \n",
    "#         nb_epoch_ae=1, nb_epoch_cf=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier.fit(x_train, y_train_oh, shuffle=True,batch_size=100, validation_data=(x_test, y_test_oh), nb_epoch=5,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# display a 2D plot of the digit classes in the latent space\n",
    "x_test_encoded = encoder.predict(x_test, batch_size=batch_size)\n",
    "print(x_test_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], c=y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train_encoded = encoder.predict(x_train, batch_size=batch_size)\n",
    "print(x_train_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(x_train_encoded[:, 0], x_train_encoded[:, 1], c=y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plotstuff.Easy3dScatter(plt, x_test_encoded[:,:3], '', s=10, c=y_test )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train_pred = classifier.predict(x_train, batch_size=batch_size)\n",
    "x_test_pred = classifier.predict(x_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(np.std(x_test_pred, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(x_train_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ambig = np.sum(np.greater(x_train_pred, 0.5) == y_train_oh, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.mean(ambig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score = classifier.evaluate(x_test, y_test_oh, batch_size=batch_size )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ambig = np.where(np.std(x_train_pred, axis=1) < 0.18)\n",
    "ambig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 8853\n",
    "print(y_train[n], np.argmax(x_train_pred[n]))\n",
    "print(np.round(x_train_pred[n]*1000))\n",
    "plt.imshow(x_train[n].reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
